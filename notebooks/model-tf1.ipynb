{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42144.371]\n",
      "Iter=10, Batch Loss=[42144.371]\n",
      "[33636.211]\n",
      "Iter=20, Batch Loss=[33636.211]\n",
      "[31504.031]\n",
      "Iter=30, Batch Loss=[31504.031]\n",
      "[40318.102]\n",
      "Iter=40, Batch Loss=[40318.102]\n",
      "[32625.121]\n",
      "Iter=50, Batch Loss=[32625.121]\n",
      "[39429.418]\n",
      "Iter=60, Batch Loss=[39429.418]\n",
      "[33164.312]\n",
      "Iter=70, Batch Loss=[33164.312]\n",
      "[29551.734]\n",
      "Iter=80, Batch Loss=[29551.734]\n",
      "[36044.0]\n",
      "Iter=90, Batch Loss=[36044.0]\n",
      "[26750.223]\n",
      "Iter=100, Batch Loss=[26750.223]\n",
      "[30645.049]\n",
      "Iter=110, Batch Loss=[30645.049]\n",
      "[24948.734]\n",
      "Iter=120, Batch Loss=[24948.734]\n",
      "[25974.473]\n",
      "Iter=130, Batch Loss=[25974.473]\n",
      "[29437.652]\n",
      "Iter=140, Batch Loss=[29437.652]\n",
      "[30375.186]\n",
      "Iter=150, Batch Loss=[30375.186]\n",
      "[28367.152]\n",
      "Iter=160, Batch Loss=[28367.152]\n",
      "[28349.021]\n",
      "Iter=170, Batch Loss=[28349.021]\n",
      "[31491.424]\n",
      "Iter=180, Batch Loss=[31491.424]\n",
      "[24604.65]\n",
      "Iter=190, Batch Loss=[24604.65]\n",
      "[25915.705]\n",
      "Iter=200, Batch Loss=[25915.705]\n",
      "[28497.473]\n",
      "Iter=210, Batch Loss=[28497.473]\n",
      "[28631.049]\n",
      "Iter=220, Batch Loss=[28631.049]\n",
      "[28239.051]\n",
      "Iter=230, Batch Loss=[28239.051]\n",
      "[28293.361]\n",
      "Iter=240, Batch Loss=[28293.361]\n",
      "[13458.11]\n",
      "Iter=250, Batch Loss=[13458.11]\n",
      "[24662.512]\n",
      "Iter=260, Batch Loss=[24662.512]\n",
      "[27752.23]\n",
      "Iter=270, Batch Loss=[27752.23]\n",
      "[26258.195]\n",
      "Iter=280, Batch Loss=[26258.195]\n",
      "[23269.508]\n",
      "Iter=290, Batch Loss=[23269.508]\n",
      "[19649.021]\n",
      "Iter=300, Batch Loss=[19649.021]\n",
      "[26924.941]\n",
      "Iter=310, Batch Loss=[26924.941]\n",
      "[17806.838]\n",
      "Iter=320, Batch Loss=[17806.838]\n",
      "[26752.691]\n",
      "Iter=330, Batch Loss=[26752.691]\n",
      "[8618.4307]\n",
      "Iter=340, Batch Loss=[8618.4307]\n",
      "[21992.322]\n",
      "Iter=350, Batch Loss=[21992.322]\n",
      "[21755.045]\n",
      "Iter=360, Batch Loss=[21755.045]\n",
      "[17628.68]\n",
      "Iter=370, Batch Loss=[17628.68]\n",
      "[21787.471]\n",
      "Iter=380, Batch Loss=[21787.471]\n",
      "[18851.18]\n",
      "Iter=390, Batch Loss=[18851.18]\n",
      "[14674.29]\n",
      "Iter=400, Batch Loss=[14674.29]\n",
      "[23264.439]\n",
      "Iter=410, Batch Loss=[23264.439]\n",
      "[5407.2876]\n",
      "Iter=420, Batch Loss=[5407.2876]\n",
      "[13580.664]\n",
      "Iter=430, Batch Loss=[13580.664]\n",
      "[17729.789]\n",
      "Iter=440, Batch Loss=[17729.789]\n",
      "[17506.688]\n",
      "Iter=450, Batch Loss=[17506.688]\n",
      "[17439.984]\n",
      "Iter=460, Batch Loss=[17439.984]\n",
      "[23040.494]\n",
      "Iter=470, Batch Loss=[23040.494]\n",
      "[20300.848]\n",
      "Iter=480, Batch Loss=[20300.848]\n",
      "[21401.627]\n",
      "Iter=490, Batch Loss=[21401.627]\n",
      "[4419.5664]\n",
      "Iter=500, Batch Loss=[4419.5664]\n",
      "[6018.0693]\n",
      "Iter=510, Batch Loss=[6018.0693]\n",
      "[17347.562]\n",
      "Iter=520, Batch Loss=[17347.562]\n",
      "[5846.4463]\n",
      "Iter=530, Batch Loss=[5846.4463]\n",
      "[9713.9219]\n",
      "Iter=540, Batch Loss=[9713.9219]\n",
      "[22821.658]\n",
      "Iter=550, Batch Loss=[22821.658]\n",
      "[16303.182]\n",
      "Iter=560, Batch Loss=[16303.182]\n",
      "[2616.2285]\n",
      "Iter=570, Batch Loss=[2616.2285]\n",
      "[11682.291]\n",
      "Iter=580, Batch Loss=[11682.291]\n",
      "[13360.291]\n",
      "Iter=590, Batch Loss=[13360.291]\n",
      "[10589.367]\n",
      "Iter=600, Batch Loss=[10589.367]\n",
      "[7565.1187]\n",
      "Iter=610, Batch Loss=[7565.1187]\n",
      "[17747.869]\n",
      "Iter=620, Batch Loss=[17747.869]\n",
      "[20893.418]\n",
      "Iter=630, Batch Loss=[20893.418]\n",
      "[1993.1832]\n",
      "Iter=640, Batch Loss=[1993.1832]\n",
      "[18646.924]\n",
      "Iter=650, Batch Loss=[18646.924]\n",
      "[15791.375]\n",
      "Iter=660, Batch Loss=[15791.375]\n",
      "[5501.1978]\n",
      "Iter=670, Batch Loss=[5501.1978]\n",
      "[20384.188]\n",
      "Iter=680, Batch Loss=[20384.188]\n",
      "[19876.061]\n",
      "Iter=690, Batch Loss=[19876.061]\n",
      "[12539.334]\n",
      "Iter=700, Batch Loss=[12539.334]\n",
      "[18945.438]\n",
      "Iter=710, Batch Loss=[18945.438]\n",
      "[6067.084]\n",
      "Iter=720, Batch Loss=[6067.084]\n",
      "[18227.053]\n",
      "Iter=730, Batch Loss=[18227.053]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/Users/jpw/Development/master_thesis/src/models/simple_feed_forward.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         sess.run(optimizer, feed_dict={X: batch_features,\n\u001b[0;32m---> 54\u001b[0;31m                                        Y: batch_targets})\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_batch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# Calculate batch loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jpw/Development/ganter/notebooks/thesis/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jpw/Development/ganter/notebooks/thesis/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jpw/Development/ganter/notebooks/thesis/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/jpw/Development/ganter/notebooks/thesis/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jpw/Development/ganter/notebooks/thesis/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run ../src/models/simple_feed_forward.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/models/feature_loader.py\n",
    "feature_loader = FeatureTensorLoader(lags=lags, check_integrity=False, batch_size=2)\n",
    "features, targets = feature_loader.load_batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "max_iter = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "n = m = 300\n",
    "lags = 2\n",
    "\n",
    "feature_loader = FeatureTensorLoader(lags=lags, check_integrity=False)\n",
    "\n",
    "def init_weights(shape):\n",
    "    weights = tf.random_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(weights)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [lags, n, m])\n",
    "Y = tf.placeholder(tf.float32, [n, m])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'w1': tf.Variable(\n",
    "        tf.random_normal([lags, n, m])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n, m])),\n",
    "}\n",
    "\n",
    "def create_network(X, weights, biases):\n",
    "    l1 = tf.nn.relu(X)\n",
    "    l1 = tf.add(tf.matmul(l1, weights['w1']), biases['b1'])\n",
    "    l1 = tf.nn.sigmoid(l1)\n",
    "    return l1\n",
    "\n",
    "network = create_network(X, weights, biases)\n",
    "square_errors = tf.nn.l2_loss(t=network)\n",
    "cost = tf.reduce_mean(square_errors)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    batch = 1\n",
    "    while batch * batch_size < max_iter:\n",
    "        batch_features, batch_targest = feature_loader(batch)\n",
    "        sess.run(optimizer, feed_dict={x: batch_features,\n",
    "                                       y: batch_targets})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run([cost], feed_dict={x: batch_features,\n",
    "                                               y: batch_targets})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Batch Loss= \" + \\\n",
    "                  \"{:.2f}\".format(loss))\n",
    "        batch += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of a simple MLP network with one hidden layer. Tested on the iris data set.\n",
    "# Requires: numpy, sklearn>=0.18.1, tensorflow>=1.0\n",
    "\n",
    "# NOTE: In order to make the code simple, we rewrite x * W_1 + b_1 = x' * W_1'\n",
    "# where x' = [x | 1] and W_1' is the matrix W_1 appended with a new row with elements b_1's.\n",
    "# Similarly, for h * W_2 + b_2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "def init_weights(shape):\n",
    "    \"\"\" Weight initialization \"\"\"\n",
    "    weights = tf.random_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(weights)\n",
    "\n",
    "def forwardprop(X, w_1, w_2):\n",
    "    \"\"\"\n",
    "    Forward-propagation.\n",
    "    IMPORTANT: yhat is not softmax since TensorFlow's softmax_cross_entropy_with_logits() does that internally.\n",
    "    \"\"\"\n",
    "    h    = tf.nn.sigmoid(tf.matmul(X, w_1))  # The \\sigma function\n",
    "    yhat = tf.matmul(h, w_2)  # The \\varphi function\n",
    "    return yhat\n",
    "\n",
    "def get_iris_data():\n",
    "    \"\"\" Read the iris data set and split them into training and test sets \"\"\"\n",
    "    iris   = datasets.load_iris()\n",
    "    data   = iris[\"data\"]\n",
    "    target = iris[\"target\"]\n",
    "\n",
    "    # Prepend the column of 1s for bias\n",
    "    N, M  = data.shape\n",
    "    all_X = np.ones((N, M + 1))\n",
    "    all_X[:, 1:] = data\n",
    "\n",
    "    # Convert into one-hot vectors\n",
    "    num_labels = len(np.unique(target))\n",
    "    all_Y = np.eye(num_labels)[target]  # One liner trick!\n",
    "    return train_test_split(all_X, all_Y, test_size=0.33, random_state=RANDOM_SEED)\n",
    "\n",
    "def main():\n",
    "    train_X, test_X, train_y, test_y = get_iris_data()\n",
    "\n",
    "    # Layer's sizes\n",
    "    x_size = train_X.shape[1]   # Number of input nodes: 4 features and 1 bias\n",
    "    h_size = 256                # Number of hidden nodes\n",
    "    y_size = train_y.shape[1]   # Number of outcomes (3 iris flowers)\n",
    "\n",
    "    # Symbols\n",
    "    X = tf.placeholder(\"float\", shape=[None, x_size])\n",
    "    y = tf.placeholder(\"float\", shape=[None, y_size])\n",
    "\n",
    "    # Weight initializations\n",
    "    w_1 = init_weights((x_size, h_size))\n",
    "    w_2 = init_weights((h_size, y_size))\n",
    "\n",
    "    # Forward propagation\n",
    "    yhat    = forwardprop(X, w_1, w_2)\n",
    "    predict = tf.argmax(yhat, axis=1)\n",
    "\n",
    "    # Backward propagation\n",
    "    cost    = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=yhat))\n",
    "    updates = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "    # Run SGD\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        # Train with each example\n",
    "        for i in range(len(train_X)):\n",
    "            sess.run(updates, feed_dict={X: train_X[i: i + 1], y: train_y[i: i + 1]})\n",
    "\n",
    "        train_accuracy = np.mean(np.argmax(train_y, axis=1) ==\n",
    "                                 sess.run(predict, feed_dict={X: train_X, y: train_y}))\n",
    "        test_accuracy  = np.mean(np.argmax(test_y, axis=1) ==\n",
    "                                 sess.run(predict, feed_dict={X: test_X, y: test_y}))\n",
    "\n",
    "        print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\"\n",
    "              % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
    "\n",
    "    sess.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
